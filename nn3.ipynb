{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "04730c20-2a7f-4d71-9564-75eea0b2eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_errors = np.power(y_true - y_pred, 2)\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    return np.sqrt(mean_squared_error)\n",
    "    \n",
    "def r_squared(y_true, y_pred):\n",
    "    sum_of_squared_residuals = np.sum(np.power(y_true - y_pred, 2))\n",
    "    mean_y = np.mean(y_true)\n",
    "    total_sum_of_squares = np.sum(np.power(y_true - mean_y, 2))\n",
    "    if total_sum_of_squares == 0:\n",
    "        return 1.0\n",
    "    return 1 - (sum_of_squared_residuals / total_sum_of_squares)\n",
    "\n",
    "    \n",
    "\n",
    "def linear(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1.0\n",
    "    return x\n",
    "\n",
    "def ReLU(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(np.float64)\n",
    "\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def logistical(x, derivative=False):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    s = 1.0/(1.0 + np.exp(-x))\n",
    "    if derivative:\n",
    "        return s*(1-s)\n",
    "    return s\n",
    "\n",
    "def softmax(z):\n",
    "    if not np.isfinite(z).all():\n",
    "        raise ValueError(\"NaN/Inf in logits\")\n",
    "    z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def binary_cross_entropy(y, y_hat, derivative=False, eps=1e-7):\n",
    "    y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "    if not derivative:\n",
    "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # This is dL/dy_hat\n",
    "    return (y_hat - y) / (y_hat * (1 - y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7d2234ad-424b-4b6a-a3d5-35d7d6253464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "\n",
    "    # Create a NeuronLayer with:\n",
    "    #     n_in input neurons\n",
    "    #     n_out output neurons\n",
    "    #     n_in * n_out connections     \n",
    "    #   Obs: if l_type == \"input\" this is just a big identity matrix :D\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,                  # Previous Layer Size / Input size\n",
    "        n_out,                 # Current Layer Size\n",
    "        l_n,\n",
    "    ):\n",
    "        self.l_n = l_n\n",
    "        self.l_size = n_out\n",
    "        self.error = np.ndarray((n_out, 1))\n",
    "        if l_n == 0:\n",
    "            self.weights = None\n",
    "            self.biases = None\n",
    "            self.l_size = n_in\n",
    "            self.Z = np.ndarray((n_in, 1))\n",
    "            self.A = np.ndarray((n_in, 1))\n",
    "            return\n",
    "            \n",
    "        self.Z = np.ndarray((n_out, 1))\n",
    "        self.A = np.ndarray((n_out, 1))\n",
    "        self.weights = np.random.normal(loc=0.0, scale=np.sqrt(2/n_in), size=(n_out, n_in))\n",
    "        self.biases = np.zeros((n_out, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        self.weights = w\n",
    "\n",
    "    def set_biases(self, b):\n",
    "        self.biases = b\n",
    "\n",
    "    def update_weights(self, learning_rate, gradW):\n",
    "        self.weights -= learning_rate*gradW\n",
    "\n",
    "    def update_biases(self, learning_rate, gradB):\n",
    "        self.biases -= learning_rate*gradB\n",
    "\n",
    "    \n",
    "    def propagate_foward(self, input, activation):\n",
    "        self.Z = np.dot(self.weights, input) + self.biases\n",
    "        self.A = activation(self.Z)\n",
    "        return self.A\n",
    "\n",
    "    \n",
    "    def propagate_backward(self, next_layer, prev_layer, m, activation):\n",
    "        self.error = np.dot(next_layer.get_weights().T, next_layer.error) * activation(self.Z, derivative=True)\n",
    "        \n",
    "        self.gradW = np.dot(self.error, prev_layer.A.T) / m\n",
    "        self.gradB = np.sum(self.error, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (self.gradW, self.gradB)\n",
    "        \n",
    "        \n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Creates an empty model. Params:\n",
    "    #     layer_sizes: array with the size of each layer [input, hidden_layer_1, hidden_layer_2, ..., output]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes,\n",
    "        activation = ReLU,\n",
    "        final_activation = linear,\n",
    "        cost_function = mse,\n",
    "        learning_rate = 0.0001,\n",
    "        model_name = \"default\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.activation = activation\n",
    "        self.final_activation = final_activation\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_of_layers = len(layer_sizes)\n",
    "        self.layers = [0] * self.n_of_layers\n",
    "        self.layers[0] = NeuronLayer(layer_sizes[0], layer_sizes[0], 0)\n",
    "        \n",
    "        for i in range(1, self.n_of_layers):\n",
    "            self.layers[i] = NeuronLayer(layer_sizes[i-1], layer_sizes[i], i)\n",
    "\n",
    "\n",
    "    def set_lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def get_layer_params(self, layer):\n",
    "        return (self.layers[layer].get_weights(), self.layers[layer].get_biases())\n",
    "\n",
    "        \n",
    "    def print_layer_params(self, layer):\n",
    "        if layer == 0:\n",
    "            print(f\"Layer 0 has {self.layers[0].l_size} inputs!\")\n",
    "            return\n",
    "            \n",
    "        w, b = self.get_layer_params(layer)\n",
    "        print(f\"Weights: ({w.shape[0]} rows & {w.shape[1]} columns) \\n\", w)\n",
    "        print(f\"Biases: ({b.shape[0]}) \\n\", b)\n",
    "\n",
    "    \n",
    "    def save_parameters(self):\n",
    "        dir_path = f\"model_params_{self.model_name}\"\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "\n",
    "            pd.DataFrame(layer.get_weights()).to_csv(f\"{dir_path}/{self.model_name}_w_{layer.l_n}\", index=False, header=False)\n",
    "            pd.DataFrame(layer.get_biases()).to_csv(f\"{dir_path}/{self.model_name}b_{layer.l_n}\", index=False, header=False)\n",
    "\n",
    "\n",
    "    def load_parameters(self, dir_path):\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "    \n",
    "            w = pd.read_csv(f\"{dir_path}/{dir_path[13:]}_w_{layer.l_n}\",\n",
    "                            header=None).values\n",
    "            b = pd.read_csv(f\"{dir_path}/{dir_path[13:]}_b_{layer.l_n}\",\n",
    "                            header=None).values\n",
    "    \n",
    "            layer.set_weights(w)\n",
    "            layer.set_biases(b)\n",
    "\n",
    "    \n",
    "    def foward(self, input):\n",
    "\n",
    "        if input.shape[0] != self.layers[0].l_size:\n",
    "            print(\"input wrong!\")\n",
    "            return None\n",
    "            \n",
    "        A_i = []\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                A_i = input\n",
    "                layer.A = A_i\n",
    "                continue\n",
    "\n",
    "            if layer.l_n == self.n_of_layers - 1:\n",
    "                A_i = layer.propagate_foward(A_i, self.final_activation)\n",
    "                return A_i\n",
    "            \n",
    "            A_i = layer.propagate_foward(A_i, self.activation)\n",
    "\n",
    "    def backpropagation(self, y, m):\n",
    "        y = np.asarray(y)\n",
    "        prediction = self.layers[-1].A   \n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "                \n",
    "            prev_layer = self.layers[layer.l_n-1]\n",
    "            \n",
    "            if layer.l_n == self.n_of_layers-1:\n",
    "                if self.final_activation is logistical and self.cost_function is binary_cross_entropy:\n",
    "                    layer.error = prediction - y\n",
    "                else:\n",
    "                    layer.error = self.cost_function(y, prediction, derivative=True) * self.final_activation(layer.Z, derivative=True)\n",
    "                    \n",
    "                layer.gradW = np.dot(layer.error, prev_layer.A.T) / m\n",
    "                layer.gradB = np.sum(layer.error, axis=1, keepdims=True) / m\n",
    "                continue\n",
    "                \n",
    "            next_layer = self.layers[layer.l_n+1]\n",
    "            gradW, gradB = layer.propagate_backward(next_layer, prev_layer, m, self.activation)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "            \n",
    "            layer.update_weights(self.learning_rate, layer.gradW)\n",
    "            layer.update_biases(self.learning_rate, layer.gradB)\n",
    "\n",
    "\n",
    "    def train(self, train_array, train_sol, epochs = 100):\n",
    "        for epoch in range(epochs):\n",
    "          for i in range(train_array.shape[0]):\n",
    "              self.foward(train_array[i].reshape(-1, 1))\n",
    "              self.backpropagation(train_sol[i].reshape(-1, 1), 1)\n",
    "            \n",
    "\n",
    "    def test(self, test_array):\n",
    "        r = np.ndarray((test_array.shape[0], self.layer_sizes[-1]))\n",
    "        for i in range(test_array.shape[0]):\n",
    "            self.foward(test_array[i].reshape(-1, 1))\n",
    "            r[i, :] = self.layers[-1].A[:, 0]\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2a6876a6-40ad-4fa7-b559-fc3fd3ac68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediana usada para corte: 840.0\n",
      "Classes no Treino: [560 545]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "url = 'https://raw.githubusercontent.com/Krumpu/Homework_ICA/main/Data-Melbourne_F.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(columns=['BOD','month', 'day','year'])\n",
    "df\n",
    "df_y = df[['COD']].copy()\n",
    "df_x = df.drop(columns=['COD'])\n",
    "\n",
    "treino_x, teste_x, treino_y, teste_y = train_test_split(df_x,df_y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "assimetria = df.drop('COD', axis=1).skew()\n",
    "colunas_assimetricas =  ['VV', 'SLP', 'PP', 'avg_inflow']\n",
    "\n",
    "for colunas in colunas_assimetricas:\n",
    "  treino_x[colunas] = np.log1p(treino_x[colunas])\n",
    "  teste_x[colunas] = np.log1p(teste_x[colunas])\n",
    "\n",
    "mediana = treino_y['COD'].median()\n",
    "\n",
    "\n",
    "y_train_cls = (treino_y['COD'] > mediana).astype(int).values\n",
    "y_test_cls = (teste_y['COD'] > mediana).astype(int).values\n",
    "print(f\"Mediana usada para corte: {mediana}\")\n",
    "print(f\"Classes no Treino: {np.bincount(y_train_cls)}\")\n",
    "\n",
    "\n",
    "#Normalização\n",
    "mean = treino_x.mean()\n",
    "std = treino_x.std()\n",
    "\n",
    "X_train_norm = ((treino_x - mean) / std).values\n",
    "X_test_norm = ((teste_x - mean) / std).values\n",
    "\n",
    "X_train_bias = np.c_[np.ones((len(X_train_norm), 1)), X_train_norm]\n",
    "X_test_bias = np.c_[np.ones((len(X_test_norm), 1)), X_test_norm]\n",
    "\n",
    "# FUNÇÕES DE AVALIAÇÃO\n",
    "def calcular_acuracia(y_real, y_pred):\n",
    "    return np.mean(y_real == y_pred)\n",
    "\n",
    "def matriz_confusao_manual(y_real, y_pred):\n",
    "    TP = np.sum((y_real == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_real == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_real == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_real == 1) & (y_pred == 0))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "72e32afc-b77d-4c92-bfae-1c48200943ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando Rede Neural...\n"
     ]
    }
   ],
   "source": [
    "print(\"Treinando Rede Neural...\")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    [X_train_bias.shape[1], 32, 128, 64, 20, 1], \n",
    "    activation=ReLU,\n",
    "    final_activation=logistical, \n",
    "    cost_function=binary_cross_entropy,\n",
    "    learning_rate=0.001, \n",
    "    model_name=\"xxx_mk6\"\n",
    ")\n",
    "\n",
    "nn.train(X_train_bias, y_train_cls, epochs=5000)\n",
    "nn.save_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0a6dfe77-58cf-4036-98df-490276579f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da Rede Neural: 83.39%\n",
      "Matriz de Confusão (Rede Neural):\n",
      " [[104  22]\n",
      " [ 24 127]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_nn = nn.test(X_test_bias)\n",
    "y_pred_cls = (y_pred_nn.squeeze() >= 0.5).astype(int)\n",
    "\n",
    "acc_log = calcular_acuracia(y_test_cls, y_pred_cls)\n",
    "cm_log = matriz_confusao_manual(y_test_cls, y_pred_cls)\n",
    "\n",
    "print(f\"Acurácia da Rede Neural: {acc_log*100:.2f}%\")\n",
    "print(\"Matriz de Confusão (Rede Neural):\\n\", cm_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dce4b-7481-45ce-a56f-e84cdef8d2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
