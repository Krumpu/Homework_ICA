{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04730c20-2a7f-4d71-9564-75eea0b2eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_errors = np.power(y_true - y_pred, 2)\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    return np.sqrt(mean_squared_error)\n",
    "    \n",
    "def r_squared(y_true, y_pred):\n",
    "    sum_of_squared_residuals = np.sum(np.power(y_true - y_pred, 2))\n",
    "    mean_y = np.mean(y_true)\n",
    "    total_sum_of_squares = np.sum(np.power(y_true - mean_y, 2))\n",
    "    if total_sum_of_squares == 0:\n",
    "        return 1.0\n",
    "    return 1 - (sum_of_squared_residuals / total_sum_of_squares)\n",
    "\n",
    "def linear(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1.0\n",
    "    return x\n",
    "\n",
    "def ReLU(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def logistical(x, derivative=False):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    s = 1.0/(1.0 + np.exp(-x))\n",
    "    if derivative:\n",
    "        return s*(1-s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2234ad-424b-4b6a-a3d5-35d7d6253464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "\n",
    "    # Create a NeuronLayer with:\n",
    "    #     n_in input neurons\n",
    "    #     n_out output neurons\n",
    "    #     n_in * n_out connections     \n",
    "    #   Obs: if l_type == \"input\" this is just a big identity matrix :D\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,                  # Previous Layer Size / Input size\n",
    "        n_out,                 # Current Layer Size\n",
    "        l_n,\n",
    "    ):\n",
    "        self.l_n = l_n\n",
    "        self.l_size = n_out\n",
    "        self.error = np.ndarray((n_out, 1))\n",
    "        if l_n == 0:\n",
    "            self.weights = None\n",
    "            self.biases = None\n",
    "            self.l_size = n_in\n",
    "            self.Z = np.ndarray((n_in, 1))\n",
    "            self.A = np.ndarray((n_in, 1))\n",
    "            return\n",
    "            \n",
    "        self.Z = np.ndarray((n_out, 1))\n",
    "        self.A = np.ndarray((n_out, 1))\n",
    "        self.weights = np.random.normal(loc=0.0, scale=np.sqrt(2/n_in), size=(n_out, n_in))\n",
    "        self.biases = np.zeros((n_out, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        self.weights = w\n",
    "\n",
    "    def set_biases(self, b):\n",
    "        self.biases = b\n",
    "\n",
    "    def update_weights(self, learning_rate, gradW):\n",
    "        self.weights -= learning_rate*gradW\n",
    "\n",
    "    def update_biases(self, learning_rate, gradB):\n",
    "        self.biases -= learning_rate*gradB\n",
    "\n",
    "    \n",
    "    def propagate_foward(self, input, activation):\n",
    "        self.Z = np.dot(self.weights, input) + self.biases\n",
    "        self.A = activation(self.Z)\n",
    "        return self.A\n",
    "\n",
    "    \n",
    "    def propagate_backward(self, next_layer, prev_layer, m, activation):\n",
    "        self.error = np.dot(next_layer.get_weights().transpose(), next_layer.error) * activation(self.Z, derivative=True)\n",
    "        dZ = self.error * 1.0\n",
    "        self.gradW = np.dot(dZ, prev_layer.A.T) / m\n",
    "        self.gradB = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (self.gradW, self.gradB)\n",
    "        \n",
    "        \n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Creates an empty model. Params:\n",
    "    #     layer_sizes: array with the size of each layer [input, hidden_layer_1, hidden_layer_2, ..., output]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes,\n",
    "        activation = ReLU,\n",
    "        final_activation = linear,\n",
    "        learning_rate = 0.0001,\n",
    "        model_name = \"default\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.activation = activation\n",
    "        self.final_activation = final_activation\n",
    "        #self.cost_function = cost_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_of_layers = len(layer_sizes)\n",
    "        self.layers = [0] * self.n_of_layers\n",
    "        self.layers[0] = NeuronLayer(layer_sizes[0], layer_sizes[0], 0)\n",
    "        \n",
    "        for i in range(1, self.n_of_layers):\n",
    "            self.layers[i] = NeuronLayer(layer_sizes[i-1], layer_sizes[i], i)\n",
    "\n",
    "\n",
    "    def set_lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "    def get_layer_params(self, layer):\n",
    "        return (self.layers[layer].get_weights(), self.layers[layer].get_biases())\n",
    "\n",
    "        \n",
    "    def print_layer_params(self, layer):\n",
    "        if layer == 0:\n",
    "            print(f\"Layer 0 has {self.layers[0].l_size} inputs!\")\n",
    "            return\n",
    "            \n",
    "        w, b = self.get_layer_params(layer)\n",
    "        print(f\"Weights: ({w.shape[0]} rows & {w.shape[1]} columns) \\n\", w)\n",
    "        print(f\"Biases: ({b.shape[0]}) \\n\", b)\n",
    "\n",
    "    \n",
    "    def save_parameters(self):\n",
    "        dir_path = f\"model_params_{self.model_name}\"\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "\n",
    "            pd.DataFrame(layer.get_weights()).to_csv(f\"{dir_path}/{self.model_name}_w_{layer.l_n}\", index=False, header=False)\n",
    "            pd.DataFrame(layer.get_biases()).to_csv(f\"{dir_path}/{self.model_name}b_{layer.l_n}\", index=False, header=False)\n",
    "\n",
    "\n",
    "    def load_parameters(self, dir_path):\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "    \n",
    "            w = pd.read_csv(f\"{dir_path}/{dir_path[13:]}_w_{layer.l_n}\",\n",
    "                            header=None).values\n",
    "            b = pd.read_csv(f\"{dir_path}/{dir_path[13:]}_b_{layer.l_n}\",\n",
    "                            header=None).values\n",
    "    \n",
    "            layer.set_weights(w)\n",
    "            layer.set_biases(b)\n",
    "\n",
    "    \n",
    "    def foward(self, input):\n",
    "\n",
    "        if input.shape[0] != self.layers[0].l_size:\n",
    "            print(\"input wrong!\")\n",
    "            return None\n",
    "            \n",
    "        A_i = []\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                A_i = input\n",
    "                layer.A = A_i\n",
    "                continue\n",
    "\n",
    "            if layer.l_n == self.n_of_layers - 1:\n",
    "                A_i = layer.propagate_foward(A_i, self.final_activation)\n",
    "                return A_i\n",
    "            \n",
    "            A_i = layer.propagate_foward(A_i, self.activation)\n",
    "\n",
    "    def backpropagation(self, y, m):\n",
    "        y = np.asarray(y)\n",
    "        prediction = self.layers[-1].A   \n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "                \n",
    "            prev_layer = self.layers[layer.l_n-1]\n",
    "            \n",
    "            if layer.l_n == self.n_of_layers-1:\n",
    "                layer.error  = prediction - y\n",
    "                layer.gradW = np.dot(layer.error, prev_layer.A.transpose()) / m\n",
    "                layer.gradB = np.sum(layer.error, axis=1, keepdims=True) / m\n",
    "                continue\n",
    "                \n",
    "            next_layer = self.layers[layer.l_n+1]\n",
    "            gradW, gradB = layer.propagate_backward(next_layer, prev_layer, m, self.activation)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if layer.l_n == 0:\n",
    "                continue\n",
    "            \n",
    "            layer.update_weights(self.learning_rate, layer.gradW)\n",
    "            layer.update_biases(self.learning_rate, layer.gradB)\n",
    "\n",
    "\n",
    "    def train(self, train_df, train_sol, epochs = 100):\n",
    "        for epoch in range(epochs):\n",
    "          for i in range(train_df.shape[0]):\n",
    "              self.foward(train_df.iloc[i].values.reshape(-1, 1))\n",
    "              self.backpropagation(train_sol.iloc[i].values.reshape(-1, 1), 1)\n",
    "            \n",
    "\n",
    "    def test(self, test_df):\n",
    "        r = []\n",
    "        for i in range(test_df.shape[0]):\n",
    "            self.foward(test_df.iloc[i].values.reshape(-1, 1))\n",
    "            r.append(self.layers[-1].A.item())\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fab273f-a2d8-4238-8354-e8d9da31a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.43529830454 0.5670763736921033\n"
     ]
    }
   ],
   "source": [
    "# Importação do conjunto de dados\n",
    "url = 'https://raw.githubusercontent.com/Krumpu/Homework_ICA/main/Data-Melbourne_F.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(columns=['BOD','month', 'day','year'])\n",
    "\n",
    "df_y = df[['COD']].copy()\n",
    "df_x = df.drop(columns=['COD'])\n",
    "\n",
    "# Divisão dos dados (80% para treino e 20% para teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "def df_z_normalize(df):\n",
    "    mean = df.mean(axis=0)\n",
    "    std_dev = df.std(axis=0)\n",
    "\n",
    "    std_dev[std_dev == 0] = 1.0 \n",
    "    \n",
    "    df_normalized = (df - mean) / std_dev\n",
    "    \n",
    "    return df_normalized, mean, std_dev\n",
    "\n",
    "df_train_X, trxMean, trxstd = df_z_normalize(X_train)\n",
    "df_train_Y, tryMean, trystd = df_z_normalize(y_train)\n",
    "df_test_X = (X_test - trxMean) / trxstd\n",
    "df_test_Y = (y_test - tryMean) / trystd\n",
    "\n",
    "\n",
    "nn = NeuralNetwork([df_train_X.shape[1], 32, 64, 20, 1], final_activation=linear, learning_rate=0.0001, model_name=\"mk4\")\n",
    "\n",
    "# This may take a while.... like 90 seconds\n",
    "nn.train(df_train_X, df_train_Y, epochs=200)\n",
    "nn.set_lr(0.00001)\n",
    "nn.train(df_train_X, df_train_Y, epochs=200)\n",
    "r = nn.test(df_test_X)\n",
    "\n",
    "pred = np.array(r).reshape(-1,1)\n",
    "pred_unscaled = pred * trystd.values + tryMean.values\n",
    "\n",
    "v_rmse , v_r_squared = rmse(y_test.values, pred_unscaled), r_squared(y_test.values, pred_unscaled)\n",
    "\n",
    "print(v_rmse, v_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6343269-1a59-486b-92bd-93c87d3169fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4232cec2-b449-423c-a7e7-3f8b517e1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.43529830453998 0.5670763736921034\n"
     ]
    }
   ],
   "source": [
    "mm = NeuralNetwork([df_train_X.shape[1], 32, 64, 20, 1], final_activation=linear, learning_rate=0.0001, model_name=\"mk5\")\n",
    "mm.load_parameters(\"model_params_mk4\")\n",
    "r = mm.test(df_test_X)\n",
    "\n",
    "pred = np.array(r).reshape(-1,1)\n",
    "pred_unscaled = pred * trystd.values + tryMean.values\n",
    "\n",
    "v_rmse , v_r_squared = rmse(y_test.values, pred_unscaled), r_squared(y_test.values, pred_unscaled)\n",
    "\n",
    "print(v_rmse, v_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfc13a-b37a-4344-820a-346277c5c010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
